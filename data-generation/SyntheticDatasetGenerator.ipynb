{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashwath-tech/llama-3.2-grumpy-it-finetune/blob/main/data-generation/SyntheticDatasetGenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mc7P9pugM2D8"
      },
      "outputs": [],
      "source": [
        "# installs\n",
        "!pip install -q json-repair\n",
        "!pip install -q --upgrade bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mfDWZAucEoH"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline\n",
        "from google.colab import userdata\n",
        "from datasets import load_dataset\n",
        "from diffusers import DiffusionPipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import random\n",
        "import json\n",
        "import re\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXENxrnF0GMK"
      },
      "outputs": [],
      "source": [
        "# login\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tifK7shVziyA"
      },
      "outputs": [],
      "source": [
        "# quantisation configuration\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRsfJ3Pxy5i-"
      },
      "outputs": [],
      "source": [
        "# get model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", device_map=\"auto\", quantization_config = quant_config)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CwUN1CTSOD2"
      },
      "outputs": [],
      "source": [
        "topics = [\n",
        "    # --- LINUX & SYSTEMS ---\n",
        "    \"Why 'chmod 777' is bad practice\",\n",
        "    \"How to kill a Zombie process that won't die\",\n",
        "    \"Disk space full but 'du' shows space available (deleted open files)\",\n",
        "    \"Difference between hard links and soft links\",\n",
        "    \"Why 'sudo rm -rf /' is blocked\",\n",
        "    \"Understanding Linux file permissions (rwx)\",\n",
        "    \"Using 'tail -f' vs 'less' for log files\",\n",
        "    \"Why the cron job didn't run (environment variables)\",\n",
        "    \"Systemd service failing to start (exit code 1)\",\n",
        "    \"High Load Average vs High CPU usage\",\n",
        "\n",
        "    # --- GIT & VERSION CONTROL ---\n",
        "    \"Fixing a 'Detached HEAD' state in Git\",\n",
        "    \"Resolving a merge conflict in package-lock.json\",\n",
        "    \"Why you shouldn't force push to main\",\n",
        "    \"Gitignore file not ignoring files that are already tracked\",\n",
        "    \"Difference between 'git merge' and 'git rebase'\",\n",
        "    \"Recovering a deleted branch with 'git reflog'\",\n",
        "    \"Squashing commits before a pull request\",\n",
        "    \"Why 'git stash' pop caused a conflict\",\n",
        "    \"Submodule initialization errors\",\n",
        "    \"Committing API keys to a public repo (and how to fix it)\",\n",
        "\n",
        "    # --- DOCKER & KUBERNETES ---\n",
        "    \"Docker container exits immediately (PID 1 issue)\",\n",
        "    \"Why 'localhost' inside Docker isn't the host machine\",\n",
        "    \"Docker volume mounting creates an empty directory\",\n",
        "    \"Reducing Docker image size (multi-stage builds)\",\n",
        "    \"Kubernetes Pod stuck in 'CrashLoopBackOff'\",\n",
        "    \"Difference between a Pod and a Deployment\",\n",
        "    \"Why the container is OOMKilled (Out of Memory)\",\n",
        "    \"Connecting two containers on the same bridge network\",\n",
        "    \"Docker Compose 'depends_on' vs actual service readiness\",\n",
        "    \"Persisting data in a stateless container\",\n",
        "\n",
        "    # --- NETWORKING & WEB ---\n",
        "    \"CORS errors (Access-Control-Allow-Origin)\",\n",
        "    \"Why DNS changes take 48 hours (TTL propagation)\",\n",
        "    \"Difference between TCP and UDP\",\n",
        "    \"Why the SSL certificate is marked 'Untrusted'\",\n",
        "    \"HTTP 401 vs 403 Forbidden\",\n",
        "    \"Port 80 vs Port 443 redirection\",\n",
        "    \"Debugging a 502 Bad Gateway error\",\n",
        "    \"Why 'ping' works but 'curl' times out (Firewall)\",\n",
        "    \"Understanding CIDR notation (e.g., /24)\",\n",
        "    \"Websocket connection failing (Upgrade header)\",\n",
        "\n",
        "    # --- DATABASE (SQL & NO-SQL) ---\n",
        "    \"Why 'SELECT *' is killing the database performance\",\n",
        "    \"The N+1 query problem in ORMs\",\n",
        "    \"Fixing a database deadlock situation\",\n",
        "    \"SQL Injection risks in raw queries\",\n",
        "    \"Why the index isn't being used (Full Table Scan)\",\n",
        "    \"Difference between INNER JOIN and LEFT JOIN\",\n",
        "    \"ACID properties in transactions\",\n",
        "    \"Why the database connection pool is full\",\n",
        "    \"Redis cache eviction policies (LRU)\",\n",
        "    \"Migration script failing due to Foreign Key constraint\",\n",
        "\n",
        "    # --- CODING & PYTHON ---\n",
        "    \"Python circular import errors\",\n",
        "    \"Mutable default arguments in Python functions\",\n",
        "    \"Why floating point math is wrong (0.1 + 0.2 != 0.3)\",\n",
        "    \"Memory leaks in long-running Node.js processes\",\n",
        "    \"Blocking the Event Loop in JavaScript\",\n",
        "    \"Dependency hell (pip/npm version conflicts)\",\n",
        "    \"Encoding issues (UTF-8 vs ASCII)\",\n",
        "    \"Race conditions in multi-threaded code\",\n",
        "    \"Recursion depth exceeded errors\",\n",
        "    \"Why you should use Virtual Environments (venv)\",\n",
        "]\n",
        "stupid_distractions = [\n",
        "    \"Do you think my wireless mouse is dying because I haven't changed the batteries in two years?\",\n",
        "    \"Can you log into the building's mainframe and redirect the AC vent? It's blowing on my left shoulder.\",\n",
        "    \"I deleted the 'Internet' icon from my desktop. Does that mean the whole company is offline now?\",\n",
        "    \"Why can't I find Comic Sans in the email dropdown? Can you install it on the mail server for me?\",\n",
        "    \"I unplugged a grey cable under my desk to charge my phone. Is that why the servers are beeping?\",\n",
        "    \"The break room coffee switched to dark roast. Does IT have a budget to fix this injustice?\",\n",
        "    \"Since you're good with 'computers,' can you look at my kid's slow iPad during lunch?\",\n",
        "    \"The printer isn't working. I haven't tried to print yet, but I have a 'feeling' it's going to jam.\",\n",
        "    \"I found a random USB drive in the parking lot. Should I plug it in to see who it belongs to?\",\n",
        "    \"My monitor is too low. Do you have any old phone books I can use as a stand?\",\n",
        "    \"Which flavor of chips in the break room provides the best brain power for coding?\",\n",
        "    \"Can you help me move my desk closer to the window? I need more 'natural light' for my RAM.\",\n",
        "    \"Is it okay if I use 'Password123' for everything? It’s the only one I can remember.\",\n",
        "    \"Why is my cursor so big today? Did the 'Internet' get wider overnight?\",\n",
        "    \"I spilled a little bit of kale smoothie on my laptop. Should I put it in a bowl of rice?\",\n",
        "    \"Does the Wi-Fi work better if I sit closer to the router, or should I hold my laptop up high?\",\n",
        "    \"Can you teach me how to Excel? I told the boss I was an expert, but I don't know what a 'cell' is.\",\n",
        "    \"The keyboard is making too much noise. Can you make the keys 'squishier' for me?\",\n",
        "    \"I saw a movie where a hacker used a black screen with green text. Can you make my Word look like that?\",\n",
        "    \"Is it true that if I don't restart my computer every night, the 'bits' get tired and slow down?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_UtE9VUGu1P"
      },
      "outputs": [],
      "source": [
        "def first_model_message():\n",
        "    if not topics: return None\n",
        "\n",
        "    selected_topic = random.choice(topics)\n",
        "    stupid_topic = random.choice(stupid_distractions)\n",
        "\n",
        "    scenario_type = random.choice([\"EARLY\", \"MIDDLE\", \"LATE\", \"NEVER\"])\n",
        "\n",
        "    if scenario_type == \"EARLY\":\n",
        "        structure = f\"\"\"\n",
        "        - Turn 1 (User): Asks about {selected_topic} but mixes in a stupid comment about {stupid_topic}.\n",
        "        - Turn 2 (Manager): Ignores the tech part, mocks the {stupid_topic} comment, then reluctantly answers the tech part.\n",
        "        - Turn 3 (User): Apologizes and asks a clean follow-up on {selected_topic}.\n",
        "        - Turn 4 (Manager): Grumpy technical deep-dive.\n",
        "        \"\"\"\n",
        "\n",
        "    elif scenario_type == \"MIDDLE\":\n",
        "        structure = f\"\"\"\n",
        "        - Turn 1 (User): Legitimate question about {selected_topic}.\n",
        "        - Turn 2 (Manager): Condescending technical explanation.\n",
        "        - Turn 3 (User): Abruptly interrupts to ask about {stupid_topic}.\n",
        "        - Turn 4 (Manager): Explodes with sarcasm, then pivots back to {selected_topic}.\n",
        "        \"\"\"\n",
        "\n",
        "    elif scenario_type == \"LATE\":\n",
        "        structure = f\"\"\"\n",
        "        - Turn 1 (User): Legitimate question about {selected_topic}.\n",
        "        - Turn 2 (Manager): Condescending technical explanation.\n",
        "        - Turn 3 (User): Follow-up technical question.\n",
        "        - Turn 4 (Manager): Even more complex explanation.\n",
        "        - Turn 5 (User): \"Oh thanks! By the way, {stupid_topic}?\"\n",
        "        - Turn 6 (Manager): Final angry shutdown. Ends conversation.\n",
        "        \"\"\"\n",
        "\n",
        "    else:\n",
        "        structure = f\"\"\"\n",
        "        - Turn 1 (User): Confused question about {selected_topic}.\n",
        "        - Turn 2 (Manager): Sarcastic but accurate explanation.\n",
        "        - Turn 3 (User): Ask for clarification on a specific detail.\n",
        "        - Turn 4 (Manager): Mocking the user's lack of knowledge, but explaining the detail perfectly.\n",
        "        - Turn 5 (User): \"Okay, I think I get it.\"\n",
        "        - Turn 6 (Manager): \"Good. Now stop bothering me.\"\n",
        "        \"\"\"\n",
        "\n",
        "    system_prompt = f\"You are a specialist dialogue generator. SCENARIO: {scenario_type} Interruption.\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    Generate a conversation between a Clueless User and a Grumpy IT Manager.\n",
        "    TOPIC: {selected_topic}\n",
        "    DISTRACTION: {stupid_topic} (Only use if scenario calls for it)\n",
        "\n",
        "    FLOW TO GENERATE:\n",
        "    {structure}\n",
        "\n",
        "    FORMAT: Use 'User: [Text]' and 'Manager: [Text]'.\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAOReNEQ7mrI"
      },
      "outputs": [],
      "source": [
        "def second_model_message(raw_answer):\n",
        "    second_system_prompt = '''You are a synthetic data formatter.\n",
        "    Convert raw dialogue into a SINGLE JSON object.\n",
        "\n",
        "    RULES:\n",
        "    - Output ONLY valid JSON.\n",
        "    - Do NOT use placeholders. Use actual text.\n",
        "    - Do NOT repeat turns.\n",
        "    '''\n",
        "\n",
        "    second_user_prompt = f'''\n",
        "    Extract the dialogue into this JSON structure.\n",
        "    RAW DIALOGUE:\n",
        "    {raw_answer}\n",
        "\n",
        "    JSON STRUCTURE:\n",
        "    {{\n",
        "      \"messages\": [\n",
        "        {{\"role\": \"system\", \"content\": \"You are a highly competent but grumpy and sarcastic IT specialist who provides technically correct answers.\"}},\n",
        "        {{\"role\": \"user\", \"content\": \"...\"}},\n",
        "        {{\"role\": \"assistant\", \"content\": \"...\"}},\n",
        "        {{\"role\": \"user\", \"content\": \"...\"}},\n",
        "        {{\"role\": \"assistant\", \"content\": \"...\"}},\n",
        "        {{\"role\": \"user\", \"content\": \"...\"}},\n",
        "        {{\"role\": \"assistant\", \"content\": \"...\"}}\n",
        "      ]\n",
        "    }}\n",
        "    REMEMBER THE ROLES PROPERLY AND DONT CHANGE ANYTHING\n",
        "    OUTPUT ONLY THE JSON.\n",
        "    '''\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": second_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": second_user_prompt}\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=True, return_tensors=\"pt\", add_generation_prompt=True).to(model.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDEHAdDM4n6S"
      },
      "outputs": [],
      "source": [
        "filename = \"grumpy_it_dataset.jsonl\"\n",
        "\n",
        "# NOTE: if generation stops in between, check the file to see how many rows have been made and change the range from that row\n",
        "\n",
        "for i in range(500):\n",
        "    print(\"--------------------------------------------\")\n",
        "    print(f\"Loop number {i+1}\")\n",
        "\n",
        "    # Generation\n",
        "    tokenized_prompt = first_model_message()\n",
        "    if tokenized_prompt is None: break\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **tokenized_prompt,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.85,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    raw_answer = tokenizer.decode(outputs[0][tokenized_prompt['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "    # Formatting\n",
        "    second_prompt = second_model_message(raw_answer)\n",
        "    outputs = model.generate(\n",
        "        **second_prompt,\n",
        "        max_new_tokens=1200,\n",
        "        temperature=0.1,\n",
        "        repetition_penalty=1.1,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    raw_answer2 = tokenizer.decode(outputs[0][second_prompt['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "    # Extraction\n",
        "    all_fragments = []\n",
        "    clean_text = re.sub(r'^(assistant|json|output)[:\\s]*', '', raw_answer2, flags=re.IGNORECASE).strip()\n",
        "    clean_text = re.sub(r'```json|```', '', clean_text).strip()\n",
        "\n",
        "    try:\n",
        "        data = json.loads(clean_text)\n",
        "        if \"messages\" in data: all_fragments = data[\"messages\"]\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"⚠️ JSON Decode Error. Attempting regex recovery...\")\n",
        "\n",
        "    # Cleaning\n",
        "    clean_dialogue = []\n",
        "\n",
        "    # Define Persona System Prompt\n",
        "    sys_msg = {\"role\": \"system\", \"content\": \"You are a highly competent but grumpy and sarcastic IT specialist who provides technically correct answers.\"}\n",
        "\n",
        "    if all_fragments:\n",
        "        for m in all_fragments:\n",
        "            content = m.get(\"content\", \"\").strip()\n",
        "            role = m.get(\"role\", \"\").lower()\n",
        "\n",
        "            # Ghost Turns & Empty Content\n",
        "            if not content or content.lower() in [\"assistant\", \"user\", \"system\", \"...\", \"turn 1\", \"turn 2\"]:\n",
        "                continue\n",
        "\n",
        "            # Skip existing System roles\n",
        "            if role == \"system\":\n",
        "                continue\n",
        "\n",
        "            # Normalize Role Names\n",
        "            actual_role = \"user\" if role == \"user\" else \"assistant\"\n",
        "\n",
        "            # If the new role is the same as the previous role, glue them together\n",
        "            if clean_dialogue and clean_dialogue[-1][\"role\"] == actual_role:\n",
        "                clean_dialogue[-1][\"content\"] += \"\\n\" + content\n",
        "                continue\n",
        "\n",
        "            # If roles are different, append as a new message\n",
        "            clean_dialogue.append({\"role\": actual_role, \"content\": content})\n",
        "\n",
        "        # --- FINAL CHECKS ---\n",
        "\n",
        "        # Sequence Guard (Must start with User)\n",
        "        if clean_dialogue and clean_dialogue[0][\"role\"] == \"assistant\":\n",
        "            clean_dialogue.pop(0)\n",
        "\n",
        "        # Truncation Guard (Remove incomplete final turn)\n",
        "        if clean_dialogue and clean_dialogue[-1][\"role\"] == \"assistant\":\n",
        "            last_text = clean_dialogue[-1][\"content\"]\n",
        "            if not last_text.endswith(('.', '!', '?', '\"', '}')):\n",
        "                print(\"⚠️ Truncation detected. Removing incomplete final turn.\")\n",
        "                clean_dialogue.pop()\n",
        "\n",
        "        # Tail Check (Must end on Assistant)\n",
        "        while clean_dialogue and clean_dialogue[-1][\"role\"] == \"user\":\n",
        "            clean_dialogue.pop()\n",
        "    # Save\n",
        "    if len(clean_dialogue) >= 2:\n",
        "        final_row = {\"messages\": [sys_msg] + clean_dialogue}\n",
        "\n",
        "        with open(filename, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(json.dumps(final_row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "        print(f\"✅ Saved row with {len(clean_dialogue)} turns.\")\n",
        "    else:\n",
        "        print(\"❌ Failed: Conversation invalid or too short.\")\n",
        "\n",
        "    # Clear\n",
        "    del outputs, raw_answer, raw_answer2, tokenized_prompt, second_prompt\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"json\", data_files=\"grumpy_it_dataset.jsonl\")\n",
        "\n",
        "# Push to Hub\n",
        "dataset.push_to_hub(\"XXXXX/grumpy-it-dataset\", private=False) #insert name"
      ],
      "metadata": {
        "id": "HjLQI3egLidp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}