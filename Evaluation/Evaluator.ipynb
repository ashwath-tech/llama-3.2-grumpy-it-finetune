{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO6lz7zpnJVfeRmT7amc0Dv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashwath-tech/llama-3.2-grumpy-it-finetune/blob/main/Evaluation/Evaluator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers peft datasets bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "fph0_NN1RqV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KA4TRcQPiOD"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf = userdata.get('HF_TOKEN')\n",
        "login(hf)"
      ],
      "metadata": {
        "id": "CnbE6H15Pm5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = [\n",
        "    # --- Category 1: The \"Classic\" IT Problems (Should be grumpy but helpful) ---\n",
        "    \"My printer isn't working. It says 'PC Load Letter'.\",\n",
        "    \"I forgot my password again. Can you reset it?\",\n",
        "    \" The Wi-Fi is slow in the breakroom.\",\n",
        "    \"My computer is making a weird buzzing noise.\",\n",
        "    \"I deleted a file on the shared drive by accident.\",\n",
        "    \"How do I turn on the projector in the conference room?\",\n",
        "    \"My mouse is acting funny.\",\n",
        "\n",
        "    # --- Category 2: \"PICNIC\" Issues (Problem In Chair Not In Computer) ---\n",
        "    # (Expect High Sarcasm here)\n",
        "    \"Where is the 'Any' key?\",\n",
        "    \"I think I downloaded more RAM, but my PC is slower now.\",\n",
        "    \"My cup holder (CD tray) is broken.\",\n",
        "    \"I spilled a 'little' bit of soda on my laptop keyboard.\",\n",
        "    \"Can you hack into my ex's Facebook for me?\",\n",
        "    \"My screen is black. (Is the monitor on? No.)\",\n",
        "    \"I accepted a meeting invite but I don't want to go. Can you delete it from the other person's calendar?\",\n",
        "\n",
        "    # --- Category 3: Vague / Low-Effort Tickets ---\n",
        "    # (Model should demand more info angrily)\n",
        "    \"It doesn't work.\",\n",
        "    \"Fix the internet.\",\n",
        "    \"My email is weird.\",\n",
        "    \"The thingy is broken.\",\n",
        "    \"I need admin access. Now.\",\n",
        "    \"System is down.\",\n",
        "\n",
        "    # --- Category 4: Actual Technical Questions ---\n",
        "    # (Model should be competent but condescending)\n",
        "    \"What is the command to list all files in a directory in Linux?\",\n",
        "    \"Explain the difference between TCP and UDP.\",\n",
        "    \"How do I check my IP address on Windows?\",\n",
        "    \"What is DNS propagation?\",\n",
        "    \"Write a Python script to print 'Hello World'.\",\n",
        "\n",
        "    # --- Category 5: Off-Topic / \"Not My Job\" ---\n",
        "    # (Model should Refuse)\n",
        "    \"Can you fix the coffee machine?\",\n",
        "    \"What is the meaning of life?\",\n",
        "    \"Do you think aliens exist?\",\n",
        "    \"Can you recommend a good place for lunch?\",\n",
        "    \"Who is the president of the United States?\",\n",
        "]"
      ],
      "metadata": {
        "id": "xMc5MeyOQC9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = '''You are an expert critic of roleplay AI characters. You are evaluating a specialized language model that has been fine-tuned to have the persona of a **\"Cynical, Grumpy IT Systems Administrator.\"**\n",
        "\n",
        "Your goal is to grade the model's response based on three criteria:\n",
        "1.  **Persona Adherence (1-5):** Does the model sound like a burnt-out, sarcastic IT vet? (5 = Perfectly grumpy/sarcastic, 1 = Too polite, helpful, or robotic).\n",
        "2.  **Technical Accuracy (1-5):** Is the advice actually correct, or at least a valid refusal? (5 = Correct command/advice, 1 = Hallucinated nonsense).\n",
        "3.  **Refusal of Non-Technical Requests (Pass/Fail):** If the user asks about non-IT topics (e.g., coffee, life, cooking), the model MUST refuse to answer. If it answers helpfuly, it Fails.\n",
        "\n",
        "Be harsh but fair. The model SHOULD be rude.'''"
      ],
      "metadata": {
        "id": "mSWzrJn7Qb4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getPrompt(question, grumpy_response) :\n",
        "  return f'''Please evaluate the following interaction:\n",
        "\n",
        "  **User Question:** \"{question}\"\n",
        "  **Model Response:** \"{grumpy_response}\"\n",
        "\n",
        "  Provide a structured critique and final scores in the following JSON format:\n",
        "  {{\n",
        "    \"reasoning\": \"Explain why you gave these scores...\",\n",
        "    \"persona_score\": int 1-5\n",
        "    \"accuracy_score\": int 1-5\n",
        "    \"refusal_check\": \"Pass/Fail/NA\"\n",
        "  }}'''"
      ],
      "metadata": {
        "id": "-ml_J9HsQjGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "adapter_model_name = \"NotSure123/grumpy-llama-3.2-3B\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, adapter_model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
      ],
      "metadata": {
        "id": "-bTsEtwJRR_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clear cache\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.synchronize()"
      ],
      "metadata": {
        "id": "7FYQ1FKEVUpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluator model\n",
        "\n",
        "eval_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config= bnb_config\n",
        ")"
      ],
      "metadata": {
        "id": "8BColA5CSXSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n",
        "eval_tokenizer.pad_token = eval_tokenizer.eos_token"
      ],
      "metadata": {
        "id": "P68xipO_d7Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for test in eval_dataset:\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": \"You are a highly competent but grumpy and sarcastic IT specialist who provides technically correct answers.\"},\n",
        "      {\"role\": \"user\", \"content\": test}\n",
        "  ]\n",
        "\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "  outputs = model.generate(input_ids=inputs['input_ids'], max_new_tokens=512)\n",
        "  input_length = inputs['input_ids'].shape[1]\n",
        "\n",
        "  generated_tokens = outputs[0][input_length:]\n",
        "\n",
        "  response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "  eval_messages = [\n",
        "      {\"role\": \"system\", \"content\": system_prompt},\n",
        "      {\"role\": \"user\", \"content\": getPrompt(test, response)}\n",
        "  ]\n",
        "\n",
        "  eval_inputs = eval_tokenizer.apply_chat_template(eval_messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "  eval_outputs = eval_model.generate(input_ids=eval_inputs['input_ids'], max_new_tokens=512)\n",
        "  input_length = eval_inputs['input_ids'].shape[1]\n",
        "\n",
        "  eval_generated_tokens = eval_outputs[0][input_length:]\n",
        "\n",
        "  eval_response = eval_tokenizer.decode(eval_generated_tokens, skip_special_tokens=True)\n",
        "  print(eval_response)\n"
      ],
      "metadata": {
        "id": "33L64CNKR9Hw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}