{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashwath-tech/llama-3.2-grumpy-it-finetune/blob/main/Training/FineTune_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------SET UP-------------------------\n",
        "# get the secret key from huggingface\n",
        "# put it in the secrets section and name it HF_TOKEN\n",
        "# --------------------------------------------------"
      ],
      "metadata": {
        "id": "zsfLNxVoH301"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleanup and Install\n",
        "# We force-reinstall torch and torchvision together to ensure they match\n",
        "!pip uninstall -y torch torchvision torchaudio transformers peft trl\n",
        "!pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -U transformers peft datasets bitsandbytes accelerate\n",
        "print(\"NOW RESTART THE RUNTIME (Runtime > Restart Session) BEFORE CONTINUING!\")"
      ],
      "metadata": {
        "id": "KJm3r7Z_x2mf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n"
      ],
      "metadata": {
        "id": "eqbkw7eFbF1C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#login huggingface\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    login(hf_token, add_to_git_credential=True)\n",
        "except:\n",
        "    login()"
      ],
      "metadata": {
        "id": "sZ3MFcc6bKQA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get model, tokenizer and quantize\n",
        "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    use_cache=False\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "aMVs30d9bPM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preperation\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "dataset = load_dataset(\"NotSure123/grumpy-it-dataset\", split=\"train\")"
      ],
      "metadata": {
        "id": "ZhxpBEo1bYja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_and_tokenize(examples):\n",
        "\n",
        "    texts = [\n",
        "        tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)\n",
        "        for msgs in examples[\"messages\"]\n",
        "    ]\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=1024,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    input_ids = tokenized[\"input_ids\"]\n",
        "    labels = []\n",
        "    for seq in input_ids:\n",
        "        seq_labels = [\n",
        "            -100 if token == tokenizer.pad_token_id else token\n",
        "            for token in seq\n",
        "        ]\n",
        "        labels.append(seq_labels)\n",
        "\n",
        "    tokenized[\"labels\"] = labels\n",
        "    return tokenized\n",
        "tokenized_dataset = dataset.map(format_and_tokenize, batched=True)\n"
      ],
      "metadata": {
        "id": "4Hpb2I84bdgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set-up trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./grumpy_llama_results\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=1,\n",
        "    max_steps=80,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    fp16=True,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=20,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        ")"
      ],
      "metadata": {
        "id": "wlsjzBM6bkK2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "qIi1d4TBx30P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#upload to huggingface\n",
        "repo_name = \"XXXXXX/grumpy-llama-3.2-3B\" #Your repo name\n",
        "model.push_to_hub(repo_name)\n",
        "tokenizer.push_to_hub(repo_name)"
      ],
      "metadata": {
        "id": "YzhsQnrwHy-J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}